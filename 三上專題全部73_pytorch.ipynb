{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAhXzxIFcICo",
        "outputId": "0a40a95a-fabb-4206-d91f-650b22ef3e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TqdO7e7ndA5u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, multilabel_confusion_matrix\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EUN9tkfBdQiL"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# PART 1: 自定義 model 與 loss (PyTorch)\n",
        "# -------------------------\n",
        "\n",
        "# Feature extractor: 接受 (batch, timesteps, channels) -> 回傳 (batch, new_timesteps, out_channels)\n",
        "def build_feature_extractor(input_shape):\n",
        "    \"\"\"\n",
        "    input_shape: (timesteps, channels)\n",
        "    returns: nn.Module whose forward accepts tensor shaped (B, T, C) and returns (B, T', C_out)\n",
        "    \"\"\"\n",
        "    class FeatureExtractor(nn.Module):\n",
        "        def __init__(self, in_channels):\n",
        "            super().__init__()\n",
        "            # mirror TF Conv1D(filters=128, kernel_size=4, padding='same') + pool\n",
        "            # Use padding = (kernel_size - 1) // 2 to approximate 'same' for stride=1\n",
        "            self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=128, kernel_size=4, stride=1, padding=1)\n",
        "            self.pool_avg = nn.AvgPool1d(kernel_size=4)\n",
        "            self.conv2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=4, stride=1, padding=1)\n",
        "            self.pool_max = nn.MaxPool1d(kernel_size=4)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Expect x shape: (B, T, C). Convert to channels-first for Conv1d\n",
        "            if x.dim() == 3:\n",
        "                x = x.permute(0, 2, 1)  # -> (B, C, T)\n",
        "            elif x.dim() == 2:\n",
        "                x = x.unsqueeze(1)  # (B,1,T) if input was (B,T)\n",
        "            # convs\n",
        "            y = F.relu(self.conv1(x))\n",
        "            y = self.pool_avg(y)\n",
        "            y = F.relu(self.conv2(y))\n",
        "            y = self.pool_max(y)  # (B, C_out, T')\n",
        "            # back to (B, T', C_out)\n",
        "            y = y.permute(0, 2, 1)\n",
        "            return y\n",
        "\n",
        "    timesteps, channels = input_shape\n",
        "    return FeatureExtractor(in_channels=channels)\n",
        "\n",
        "\n",
        "# Capsule layer: 實作與你 TF 版本等價的動態路由，並使用 shared linear W (對應 add_weight)\n",
        "class CapsuleLayer(nn.Module):\n",
        "    def __init__(self, in_dim, num_capsules=10, capsule_dim=16, routing_iters=3):\n",
        "        \"\"\"\n",
        "        in_dim: dimension of each input capsule (e.g., channels after CNN)\n",
        "        num_capsules: number of output capsules\n",
        "        capsule_dim: dimension of each output capsule\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.num_capsules = num_capsules\n",
        "        self.capsule_dim = capsule_dim\n",
        "        self.routing_iters = routing_iters\n",
        "        # W: (in_dim, num_capsules * capsule_dim)\n",
        "        self.W = nn.Parameter(torch.empty(in_dim, num_capsules * capsule_dim))\n",
        "        # Xavier / glorot uniform初始化，對應 TF 的 glorot_uniform\n",
        "        nn.init.xavier_uniform_(self.W)\n",
        "\n",
        "    def squash(self, s, dim=-1, eps=1e-9):\n",
        "        squared_norm = (s ** 2).sum(dim=dim, keepdim=True)\n",
        "        scale = squared_norm / (1.0 + squared_norm)\n",
        "        return scale * s / torch.sqrt(squared_norm + eps)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: (batch, input_len, in_dim)\n",
        "        returns: (batch, num_capsules, capsule_dim)\n",
        "        \"\"\"\n",
        "        # inputs_hat = einsum('bij,jk->bik', inputs, self.W)\n",
        "        # inputs_hat shape -> (batch, input_len, num_capsules * capsule_dim)\n",
        "        inputs_hat = torch.einsum('bij,jk->bik', inputs, self.W)\n",
        "        batch_size = inputs_hat.size(0)\n",
        "        input_len = inputs_hat.size(1)\n",
        "\n",
        "        # reshape to (batch, input_len, num_capsules, capsule_dim)\n",
        "        inputs_hat = inputs_hat.view(batch_size, input_len, self.num_capsules, self.capsule_dim)\n",
        "\n",
        "        # initialize b zeros (batch, input_len, num_capsules)\n",
        "        b = torch.zeros(batch_size, input_len, self.num_capsules, device=inputs.device, dtype=inputs.dtype)\n",
        "\n",
        "        for i in range(self.routing_iters):\n",
        "            c = torch.softmax(b, dim=2)  # (batch, input_len, num_capsules)\n",
        "            # s = sum_i c_ij * inputs_hat_ij  -> (batch, num_capsules, capsule_dim)\n",
        "            s = (c.unsqueeze(-1) * inputs_hat).sum(dim=1)\n",
        "            v = self.squash(s)\n",
        "            if i < self.routing_iters - 1:\n",
        "                # agreement: sum over capsule_dim\n",
        "                # inputs_hat * v.unsqueeze(1) -> (batch, input_len, num_capsules, capsule_dim)\n",
        "                # reduce_sum over last dim -> (batch, input_len, num_capsules)\n",
        "                agreement = (inputs_hat * v.unsqueeze(1)).sum(dim=-1)\n",
        "                b = b + agreement\n",
        "        return v  # (batch, num_capsules, capsule_dim)\n",
        "\n",
        "\n",
        "# build_capsule_classifier: 接受 capsule 輸入 shape (time_steps, channels) 與超參數，回傳 nn.Module\n",
        "def build_capsule_classifier(input_shape, num_capsules=10, capsule_dim=16, num_classes=4, routing_iters=3):\n",
        "    \"\"\"\n",
        "    input_shape: tuple (time_steps, channels) representing CNN output per-sample (like Keras's dummy_output.shape[1:])\n",
        "    returns: nn.Module which accepts input tensor shaped (batch, time_steps, channels)\n",
        "    \"\"\"\n",
        "    class CapsuleClassifier(nn.Module):\n",
        "        def __init__(self, in_time, in_channels, num_capsules, capsule_dim, num_classes, routing_iters):\n",
        "            super().__init__()\n",
        "            self.capsule_layer = CapsuleLayer(in_dim=in_channels,\n",
        "                                              num_capsules=num_capsules,\n",
        "                                              capsule_dim=capsule_dim,\n",
        "                                              routing_iters=routing_iters)\n",
        "            self.flatten = nn.Flatten()\n",
        "            self.fc = nn.Linear(num_capsules * capsule_dim, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # x shape expected: (batch, time_steps, channels)\n",
        "            # pass into capsule layer (which expects (batch, input_len, in_dim) ) -> (batch, num_capsules, capsule_dim)\n",
        "            v = self.capsule_layer(x)\n",
        "            flat = self.flatten(v)\n",
        "            logits = self.fc(flat)\n",
        "            probs = torch.sigmoid(logits)  # multi-label sigmoid as in your TF model\n",
        "            return probs\n",
        "\n",
        "    time_steps, channels = input_shape\n",
        "    return CapsuleClassifier(time_steps, channels, num_capsules, capsule_dim, num_classes, routing_iters)\n",
        "\n",
        "\n",
        "# margin_loss: 與你 TF 版本等價\n",
        "def margin_loss(y_true, y_pred, m_plus=0.9, m_minus=0.1, lambd=0.5):\n",
        "    \"\"\"\n",
        "    y_true: (batch, num_classes) float (0/1)\n",
        "    y_pred: (batch, num_classes) float in [0,1] (sigmoid outputs)\n",
        "    \"\"\"\n",
        "    y_true = y_true.float()\n",
        "    y_pred = y_pred.float()\n",
        "    term1 = y_true * (torch.clamp(m_plus - y_pred, min=0.0) ** 2)\n",
        "    term2 = lambd * (1.0 - y_true) * (torch.clamp(y_pred - m_minus, min=0.0) ** 2)\n",
        "    L = term1 + term2\n",
        "    return torch.mean(torch.sum(L, dim=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa9n15VYdDXE",
        "outputId": "2de110f8-c146-4461-85ba-be152277f0e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "訓練集大小: (734, 12288, 1) (734, 4)\n",
            "測試集大小: (315, 12288, 1) (315, 4)\n"
          ]
        }
      ],
      "source": [
        "# 設置默認工作目錄\n",
        "folder_path = '/content/drive/MyDrive/20240715_大馬達收集資料'\n",
        "os.chdir(folder_path)\n",
        "\n",
        "# 定義資料夾和標籤（多標籤）\n",
        "folders = [\"齒輪不對中、軸承內斷\", \"齒輪不對中、軸承正常\", \"齒輪正常、軸承內斷\", \"齒輪正常、軸承正常\", \"齒輪磨耗、軸承內斷\", \"齒輪磨耗、軸承正常\"]\n",
        "labels_map = {\n",
        "    # 都正常 - 0 齒輪磨耗 - 1  齒輪不對中 - 2  軸承內斷 - 3\n",
        "    \"齒輪正常、軸承正常\": [0],\n",
        "    \"齒輪磨耗、軸承正常\": [1],\n",
        "    \"齒輪不對中、軸承正常\": [2],\n",
        "    \"齒輪正常、軸承內斷\": [3],\n",
        "    \"齒輪磨耗、軸承內斷\": [1, 3],\n",
        "    \"齒輪不對中、軸承內斷\": [2, 3]\n",
        "}\n",
        "\n",
        "# 初始化數據和標籤\n",
        "data = []\n",
        "fault_labels = []\n",
        "\n",
        "# 讀取每個資料夾中的CSV文件\n",
        "for folder, labels in labels_map.items():\n",
        "    for file in os.listdir(folder):\n",
        "        if file.endswith(\".csv\"):\n",
        "            filepath = os.path.join(folder, file)\n",
        "            try:\n",
        "                df = pd.read_csv(filepath, header=None)\n",
        "                if df.shape[0] > 0 and df.shape[1] > 1:\n",
        "                    data.append(df.iloc[0, 1:].values)\n",
        "                    fault_labels.append(labels)\n",
        "                else:\n",
        "                    print(f\"Skipping file {filepath}: Not enough data.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file {filepath}: {e}\")\n",
        "\n",
        "# 確認是否有收集到數據\n",
        "if not data:\n",
        "    raise ValueError(\"No valid data found. Please check the CSV files.\")\n",
        "\n",
        "# 將數據轉換為 DataFrame\n",
        "data_df = pd.DataFrame(data)\n",
        "\n",
        "# 創建多標籤矩陣\n",
        "num_classes = 4  # 0: 齒輪正常、軸承正常, 1: 齒輪磨耗, 2: 齒輪不對中, 3: 軸承內斷\n",
        "multi_label_matrix = np.zeros((len(fault_labels), num_classes), dtype=int)\n",
        "\n",
        "for i, label_list in enumerate(fault_labels):\n",
        "    for label in label_list:\n",
        "        multi_label_matrix[i, label] = 1\n",
        "\n",
        "# 分割數據集\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_df, multi_label_matrix, test_size=0.3, random_state=42)\n",
        "\n",
        "# 標準化數據\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 調整輸入數據的形狀\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# 打印數據集信息\n",
        "print(\"訓練集大小:\", X_train.shape, y_train.shape)\n",
        "print(\"測試集大小:\", X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mhr8vJG19mWW",
        "outputId": "5fc18a96-b535-4946-9301-06d9eb547c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 開始獨立計數程序 ---\n",
            "\n",
            "----- 各資料夾樣本分佈 -----\n",
            "資料夾: 齒輪正常、軸承正常\n",
            "  原始樣本數: 171\n",
            "  -> training: 118 筆\n",
            "  -> testing : 53 筆\n",
            "\n",
            "資料夾: 齒輪磨耗、軸承正常\n",
            "  原始樣本數: 181\n",
            "  -> training: 118 筆\n",
            "  -> testing : 63 筆\n",
            "\n",
            "資料夾: 齒輪不對中、軸承正常\n",
            "  原始樣本數: 178\n",
            "  -> training: 122 筆\n",
            "  -> testing : 56 筆\n",
            "\n",
            "資料夾: 齒輪正常、軸承內斷\n",
            "  原始樣本數: 176\n",
            "  -> training: 121 筆\n",
            "  -> testing : 55 筆\n",
            "\n",
            "資料夾: 齒輪磨耗、軸承內斷\n",
            "  原始樣本數: 174\n",
            "  -> training: 137 筆\n",
            "  -> testing : 37 筆\n",
            "\n",
            "資料夾: 齒輪不對中、軸承內斷\n",
            "  原始樣本數: 169\n",
            "  -> training: 118 筆\n",
            "  -> testing : 51 筆\n",
            "\n",
            "----- 總計檢查 -----\n",
            "原始總樣本數: 1049\n",
            "訓練集合計 (應為 734): 734\n",
            "測試集合計 (應為 315): 315\n",
            "訓練 + 測試 合計: 1049\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "print(\"--- 開始獨立計數程序 ---\")\n",
        "\n",
        "# --- 步驟 1: 從既有的標籤反向推導每筆資料的原始資料夾名稱 ---\n",
        "# 建立一個從 \"標籤組合\" -> \"資料夾名稱\" 的反向對應字典\n",
        "# 例如：(1, 3) -> \"齒輪磨耗、軸承內斷\"\n",
        "# 我們使用 tuple 作為 key，因為 list 不能當作字典的 key\n",
        "reverse_labels_map = {tuple(v): k for k, v in labels_map.items()}\n",
        "\n",
        "# 根據 fault_labels 列表，建立一個長度與總樣本數相同的列表\n",
        "# 其中每個元素都是該樣本對應的原始資料夾名稱\n",
        "try:\n",
        "    original_folders_reconstructed = [reverse_labels_map[tuple(lbl)] for lbl in fault_labels]\n",
        "except KeyError as e:\n",
        "    print(f\"錯誤：在反向對應時找不到鍵 {e}。請檢查您的 `labels_map` 和 `fault_labels` 是否匹配。\")\n",
        "    # 這裡可以拋出錯誤或停止執行\n",
        "    raise\n",
        "\n",
        "# --- 步驟 2: 重新模擬 train_test_split 的 \"索引\" 分割過程 ---\n",
        "# 建立一個從 0 到 N-1 的索引陣列 (N為總樣本數)\n",
        "total_samples = len(data_df)\n",
        "indices = np.arange(total_samples)\n",
        "\n",
        "# 使用與您原始碼完全相同的參數 (test_size, random_state) 來分割索引\n",
        "# 這能確保我們得到與您原始資料分割完全一致的索引結果\n",
        "train_indices, test_indices = train_test_split(\n",
        "    indices,\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# --- 步驟 3: 利用分割後的索引來計數 ---\n",
        "# 根據 train_indices 從原始資料夾列表中取出所有訓練集樣本的來源\n",
        "folders_in_train = [original_folders_reconstructed[i] for i in train_indices]\n",
        "# 根據 test_indices 取出所有測試集樣本的來源\n",
        "folders_in_test = [original_folders_reconstructed[i] for i in test_indices]\n",
        "\n",
        "# 使用 Counter 直接計算各個來源資料夾在訓練集與測試集中出現的次數\n",
        "train_counts = Counter(folders_in_train)\n",
        "test_counts = Counter(folders_in_test)\n",
        "original_counts = Counter(original_folders_reconstructed)\n",
        "\n",
        "# --- 步驟 4: 格式化並印出結果 ---\n",
        "print(\"\\n----- 各資料夾樣本分佈 -----\")\n",
        "# 依照您 labels_map 的順序來印出，確保每次結果的順序都一樣\n",
        "for folder in labels_map.keys():\n",
        "    orig_count = original_counts.get(folder, 0)\n",
        "    train_count = train_counts.get(folder, 0)\n",
        "    test_count = test_counts.get(folder, 0)\n",
        "\n",
        "    # 只有當該資料夾有數據時才印出\n",
        "    if orig_count > 0:\n",
        "        print(f\"資料夾: {folder}\")\n",
        "        print(f\"  原始樣本數: {orig_count}\")\n",
        "        print(f\"  -> training: {train_count} 筆\")\n",
        "        print(f\"  -> testing : {test_count} 筆\")\n",
        "        print(\"\")\n",
        "\n",
        "print(\"----- 總計檢查 -----\")\n",
        "print(f\"原始總樣本數: {total_samples}\")\n",
        "print(f\"訓練集合計 (應為 {len(X_train)}): {len(train_indices)}\")\n",
        "print(f\"測試集合計 (應為 {len(X_test)}): {len(test_indices)}\")\n",
        "print(f\"訓練 + 測試 合計: {len(train_indices) + len(test_indices)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdVsQWxVdZ1l",
        "outputId": "59866f45-9f97-49cd-8c5a-9d195bd347a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 - train_loss: 0.303881 - val_exact_match_acc: 11.7460%\n",
            "Epoch 2/30 - train_loss: 0.270867 - val_exact_match_acc: 17.4603%\n",
            "Epoch 3/30 - train_loss: 0.266496 - val_exact_match_acc: 7.3016%\n",
            "Epoch 4/30 - train_loss: 0.265003 - val_exact_match_acc: 28.5714%\n",
            "Epoch 5/30 - train_loss: 0.253730 - val_exact_match_acc: 28.8889%\n",
            "Epoch 6/30 - train_loss: 0.230593 - val_exact_match_acc: 29.8413%\n",
            "Epoch 7/30 - train_loss: 0.196576 - val_exact_match_acc: 28.8889%\n",
            "Epoch 8/30 - train_loss: 0.184715 - val_exact_match_acc: 28.5714%\n",
            "Epoch 9/30 - train_loss: 0.180440 - val_exact_match_acc: 32.6984%\n",
            "Epoch 10/30 - train_loss: 0.180932 - val_exact_match_acc: 40.0000%\n",
            "Epoch 11/30 - train_loss: 0.175703 - val_exact_match_acc: 18.0952%\n",
            "Epoch 12/30 - train_loss: 0.177760 - val_exact_match_acc: 28.5714%\n",
            "Epoch 13/30 - train_loss: 0.176311 - val_exact_match_acc: 28.8889%\n",
            "Epoch 14/30 - train_loss: 0.165860 - val_exact_match_acc: 29.5238%\n",
            "Epoch 15/30 - train_loss: 0.170043 - val_exact_match_acc: 47.6190%\n",
            "Epoch 16/30 - train_loss: 0.156429 - val_exact_match_acc: 51.1111%\n",
            "Epoch 17/30 - train_loss: 0.150570 - val_exact_match_acc: 70.4762%\n",
            "Epoch 18/30 - train_loss: 0.125027 - val_exact_match_acc: 68.2540%\n",
            "Epoch 19/30 - train_loss: 0.111000 - val_exact_match_acc: 63.8095%\n",
            "Epoch 20/30 - train_loss: 0.105947 - val_exact_match_acc: 72.0635%\n",
            "Epoch 21/30 - train_loss: 0.105342 - val_exact_match_acc: 76.8254%\n",
            "Epoch 22/30 - train_loss: 0.110872 - val_exact_match_acc: 69.2063%\n",
            "Epoch 23/30 - train_loss: 0.091643 - val_exact_match_acc: 71.1111%\n",
            "Epoch 24/30 - train_loss: 0.106752 - val_exact_match_acc: 56.8254%\n",
            "Epoch 25/30 - train_loss: 0.102709 - val_exact_match_acc: 80.6349%\n",
            "Epoch 26/30 - train_loss: 0.089803 - val_exact_match_acc: 80.6349%\n",
            "Epoch 27/30 - train_loss: 0.080078 - val_exact_match_acc: 71.1111%\n",
            "Epoch 28/30 - train_loss: 0.076298 - val_exact_match_acc: 73.0159%\n",
            "Epoch 29/30 - train_loss: 0.093630 - val_exact_match_acc: 78.7302%\n",
            "Epoch 30/30 - train_loss: 0.081250 - val_exact_match_acc: 85.7143%\n",
            "Number of successful predictions: 270 out of 315\n",
            "Overall Accuracy: 85.71%\n",
            "標籤 '正常(0)' 的混淆矩陣:\n",
            "[[262   0]\n",
            " [  0  53]]\n",
            "\n",
            "標籤 '齒輪磨耗(1)' 的混淆矩陣:\n",
            "[[187  28]\n",
            " [  2  98]]\n",
            "\n",
            "標籤 '齒輪不對中(2)' 的混淆矩陣:\n",
            "[[204   4]\n",
            " [ 11  96]]\n",
            "\n",
            "標籤 '軸承內斷(3)' 的混淆矩陣:\n",
            "[[172   0]\n",
            " [  0 143]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# PART 2: 模型建立、訓練與推論（直接可跑）\n",
        "# -------------------------\n",
        "\n",
        "# 假設以下變數已存在於環境中（來自你的原始資料）\n",
        "# X_train, y_train, X_test, y_test  (numpy arrays)\n",
        "# num_classes\n",
        "\n",
        "# If X_train is 2D (N, T), make it (N, T, 1)\n",
        "if X_train.ndim == 2:\n",
        "    X_train = X_train[..., np.newaxis]\n",
        "    X_test = X_test[..., np.newaxis]\n",
        "\n",
        "# input_shape as in your TF code: (timesteps, channels)\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "# 1) 建 CNN (feature extractor)\n",
        "cnn_model = build_feature_extractor(input_shape)\n",
        "\n",
        "# 2) 用 dummy input 取得 CNN 輸出 shape (time_steps_after_cnn, channels_out)\n",
        "# Prepare dummy (1, T, C) and forward through cnn_model to get shape\n",
        "with torch.no_grad():\n",
        "    dummy = torch.randn(1, input_shape[0], input_shape[1])\n",
        "    dummy_out = cnn_model(dummy)  # shape (1, new_time, out_channels)\n",
        "    capsule_input_shape = (dummy_out.shape[1], dummy_out.shape[2])  # (time_steps, channels)\n",
        "\n",
        "# 3) 建 capsule classifier (接受 cnn 的輸出格式)\n",
        "capsule_classifier = build_capsule_classifier(\n",
        "    input_shape=capsule_input_shape,\n",
        "    num_capsules=10,\n",
        "    capsule_dim=16,\n",
        "    num_classes=num_classes,\n",
        "    routing_iters=3\n",
        ")\n",
        "\n",
        "# 4) 串接 CNN 和 capsule classifier 成一個完整模型\n",
        "class DDCNNModel(nn.Module):\n",
        "    def __init__(self, cnn, capsule_clf):\n",
        "        super().__init__()\n",
        "        self.cnn = cnn\n",
        "        self.capsule_clf = capsule_clf\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x expected (batch, timesteps, channels)\n",
        "        y = self.cnn(x)  # (batch, new_time, channels_out)\n",
        "        y = self.capsule_clf(y)  # (batch, num_classes) sigmoid probs\n",
        "        return y\n",
        "\n",
        "ddcnn_model = DDCNNModel(cnn_model, capsule_classifier)\n",
        "\n",
        "# 5) training setup: optimizer, loss (use margin_loss), DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ddcnn_model.to(device)\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 30\n",
        "optimizer = torch.optim.Adam(ddcnn_model.parameters(), lr=1e-3)\n",
        "\n",
        "# prepare datasets (y as float for margin_loss)\n",
        "train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "test_ds = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 6) Training loop (使用 margin_loss)\n",
        "for epoch in range(epochs):\n",
        "    ddcnn_model.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = ddcnn_model(xb)  # (batch, num_classes), sigmoid probs\n",
        "        loss = margin_loss(yb, y_pred)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # validation: 計算 exact-match accuracy (跟你最後的 success_count 一致)\n",
        "    ddcnn_model.eval()\n",
        "    all_preds = []\n",
        "    all_trues = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            y_pred = ddcnn_model(xb)  # probs\n",
        "            all_preds.append(y_pred.cpu().numpy())\n",
        "            all_trues.append(yb.cpu().numpy())\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_trues = np.vstack(all_trues)\n",
        "    phi = 0.5\n",
        "    pred_labels = (all_preds > phi).astype(int)\n",
        "    success_count = np.sum(np.all(pred_labels == all_trues, axis=1))\n",
        "    total_count = len(all_trues)\n",
        "    val_acc = success_count / total_count\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - train_loss: {epoch_loss:.6f} - val_exact_match_acc: {val_acc:.4%}\")\n",
        "\n",
        "# 7) 訓練完成後做 predict 與分析（等價於你的 Keras 部分）\n",
        "ddcnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    y_pred = ddcnn_model(X_test_t).cpu().numpy()  # (N_test, num_classes)\n",
        "phi = 0.5\n",
        "y_pred_labels = (y_pred > phi).astype(int)\n",
        "\n",
        "# 計算總準確性（與你原本那段相同）\n",
        "success_count = np.sum(np.all(y_pred_labels == y_test, axis=1))\n",
        "total_count = len(y_test)\n",
        "print(f\"Number of successful predictions: {success_count} out of {total_count}\")\n",
        "print(f\"Overall Accuracy: {success_count / total_count:.2%}\")\n",
        "\n",
        "# 多標籤混淆矩陣 (如你原本所做)\n",
        "multi_confusion_matrix = multilabel_confusion_matrix(y_test, y_pred_labels)\n",
        "label_names = [\"正常(0)\", \"齒輪磨耗(1)\", \"齒輪不對中(2)\", \"軸承內斷(3)\"] if num_classes==4 else [f\"label_{i}\" for i in range(num_classes)]\n",
        "for i, cm in enumerate(multi_confusion_matrix):\n",
        "    print(f\"標籤 '{label_names[i]}' 的混淆矩陣:\")\n",
        "    print(cm)\n",
        "    print(\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_segK2hmBQr",
        "outputId": "90e81895-e03e-4965-cae6-ebd1f5fca66f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- 準確率評估報告 -----\n",
            "嚴格成功預測數量 (完全匹配): 270 / 315\n",
            "嚴格整體準確率 (Strict Accuracy): 85.71%\n",
            "------------------------------\n",
            "可接受的成功預測數量 (包含部分正確): 302 / 315\n",
            "可接受的整體準確率 (Acceptable Accuracy): 95.87%\n",
            "\n",
            "----- 各資料夾可接受準確率 -----\n",
            "  齒輪正常、軸承正常: 53 test samples, 100.00% accuracy\n",
            "  齒輪磨耗、軸承正常: 63 test samples, 100.00% accuracy\n",
            "  齒輪不對中、軸承正常: 56 test samples, 100.00% accuracy\n",
            "  齒輪正常、軸承內斷: 55 test samples, 100.00% accuracy\n",
            "  齒輪磨耗、軸承內斷: 37 test samples, 94.59% accuracy\n",
            "  齒輪不對中、軸承內斷: 51 test samples, 78.43% accuracy\n",
            "\n",
            "\n",
            "----- 各標籤獨立混淆矩陣 -----\n",
            "標籤 '正常(0)' 的混淆矩陣:\n",
            "[[262   0]\n",
            " [  0  53]]\n",
            "\n",
            "標籤 '齒輪磨耗(1)' 的混淆矩陣:\n",
            "[[187  28]\n",
            " [  2  98]]\n",
            "\n",
            "標籤 '齒輪不對中(2)' 的混淆矩陣:\n",
            "[[204   4]\n",
            " [ 11  96]]\n",
            "\n",
            "標籤 '軸承內斷(3)' 的混淆矩陣:\n",
            "[[172   0]\n",
            " [  0 143]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 修正後的準確率計算邏輯 ---\n",
        "\n",
        "total_count = len(y_test)\n",
        "strict_success_count = 0      # 嚴格成功計數器\n",
        "acceptable_success_count = 0  # 可接受的成功計數器\n",
        "\n",
        "# 初始化用於計算各資料夾準確率的計數器\n",
        "folder_test_count = Counter()\n",
        "folder_correct_count = Counter() # 基於 \"可接受\" 標準的正確計數\n",
        "\n",
        "# 遍歷每一筆測試資料\n",
        "for i in range(total_count):\n",
        "    y_true = y_test[i]\n",
        "    y_pred = y_pred_labels[i]\n",
        "\n",
        "    # 取得此筆測試資料的原始資料夾名稱並計數\n",
        "    folder_name = folders_in_test[i]\n",
        "    folder_test_count[folder_name] += 1\n",
        "\n",
        "    is_acceptably_correct = False\n",
        "\n",
        "    # 情況 1: 嚴格比對，預測與真實標籤完全相同\n",
        "    if np.array_equal(y_true, y_pred):\n",
        "        strict_success_count += 1\n",
        "        is_acceptably_correct = True\n",
        "    else:\n",
        "        # 情況 2: 可接受的部份正確\n",
        "        is_true_single_fault = (np.sum(y_true) == 1)\n",
        "        is_pred_composite_fault = (np.sum(y_pred) > 1)\n",
        "\n",
        "        if is_true_single_fault and is_pred_composite_fault:\n",
        "            if np.array_equal(np.bitwise_and(y_true, y_pred), y_true):\n",
        "                is_acceptably_correct = True\n",
        "\n",
        "    # 如果這筆預測被判定為 \"可接受的正確\"，則更新整體與個別資料夾的計數器\n",
        "    if is_acceptably_correct:\n",
        "        acceptable_success_count += 1\n",
        "        folder_correct_count[folder_name] += 1\n",
        "\n",
        "\n",
        "# --- 打印兩種準確率結果 ---\n",
        "\n",
        "print(\"----- 準確率評估報告 -----\")\n",
        "\n",
        "# 1. 嚴格準確率 (Strict Accuracy)\n",
        "print(f\"嚴格成功預測數量 (完全匹配): {strict_success_count} / {total_count}\")\n",
        "print(f\"嚴格整體準確率 (Strict Accuracy): {strict_success_count / total_count:.2%}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 2. 可接受準確率 (Acceptable Accuracy)\n",
        "print(f\"可接受的成功預測數量 (包含部分正確): {acceptable_success_count} / {total_count}\")\n",
        "print(f\"可接受的整體準確率 (Acceptable Accuracy): {acceptable_success_count / total_count:.2%}\")\n",
        "\n",
        "# --- 新增：計算並輸出每個資料夾的 \"可接受\" 準確率 ---\n",
        "print(\"\\n----- 各資料夾可接受準確率 -----\")\n",
        "# 依您定義的順序遍歷，確保輸出順序一致\n",
        "for folder in labels_map.keys():\n",
        "    total = folder_test_count[folder]\n",
        "    # 只有當該資料夾確實有測試樣本時才進行計算與打印\n",
        "    if total > 0:\n",
        "        correct = folder_correct_count[folder]\n",
        "        accuracy = correct / total\n",
        "        print(f\"  {folder}: {total} test samples, {accuracy:.2%} accuracy\")\n",
        "\n",
        "\n",
        "# --- 多標籤混淆矩陣 (此部分不變) ---\n",
        "print(\"\\n\\n----- 各標籤獨立混淆矩陣 -----\")\n",
        "multi_confusion_matrix = multilabel_confusion_matrix(y_test, y_pred_labels)\n",
        "\n",
        "# 為了方便解讀，我們定義標籤名稱\n",
        "label_names = [\"正常(0)\", \"齒輪磨耗(1)\", \"齒輪不對中(2)\", \"軸承內斷(3)\"]\n",
        "for i, cm in enumerate(multi_confusion_matrix):\n",
        "    print(f\"標籤 '{label_names[i]}' 的混淆矩陣:\")\n",
        "    print(cm)\n",
        "    print(\"\") # 增加間隔"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}